{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection Impact (T-tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "def t_test(original, selection):\n",
    "    \"\"\"Comparing method\"\"\"\n",
    "    def two_tailed_t_test(original, selection):\n",
    "        n_d = len(selection)\n",
    "        n_c = len(original)\n",
    "        n = min(n_d, n_c)\n",
    "        t, p = ttest_rel(original[:n], selection[:n])\n",
    "        if np.isnan(t):\n",
    "            t, p = 0, 1\n",
    "        return {\"t-stats\":t, \"p-value\":p}\n",
    "\n",
    "    def one_tailed_t_test(original, selection, direction):\n",
    "        two_tail = two_tailed_t_test(original, selection)\n",
    "        t, p_two = two_tail['t-stats'], two_tail['p-value']\n",
    "        if direction == 'positive':\n",
    "            if t > 0 :\n",
    "                p = p_two * 0.5\n",
    "            else:\n",
    "                p = 1 - p_two * 0.5\n",
    "        else:\n",
    "            if t < 0:\n",
    "                p = p_two * 0.5\n",
    "            else:\n",
    "                p = 1 - p_two * 0.5\n",
    "        return {\"t-stats\":t, \"p-value\":p}\n",
    "\n",
    "    result = {}\n",
    "    result['two_tail'] = two_tailed_t_test(original, selection)\n",
    "    result['one_tail_pos'] = one_tailed_t_test(original, selection, 'positive')\n",
    "    result['one_tail_neg'] = one_tailed_t_test(original, selection, 'negative')\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_score(original, selection):\n",
    "    alpha =  0.05\n",
    "    results = t_test(original, selection)\n",
    "    difference = 'insignificant'\n",
    "\n",
    "    if results['two_tail']['p-value'] < alpha:\n",
    "        if results['one_tail_neg']['p-value'] < alpha:\n",
    "            difference = 'positive'\n",
    "        if results['one_tail_pos']['p-value'] < alpha:\n",
    "            difference = 'negative'\n",
    "\n",
    "    return difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T-tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "Logreg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "MLP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************\n",
      "SVM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "/home/zeyang/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Define paths and initialize variables\n",
    "systems_path = ['../../results/ars-selection/CRAIGPB', '../../results/ars-selection/GLISTERPB', '../../results/ars-selection/GradMatchPB', '../../results/ars-selection/Random']\n",
    "ratio_path = ['/ars_0.05', '/ars_0.1', '/ars_0.2', '/ars_0.3', '/ars_0.5']\n",
    "models = ['Logreg', 'MLP', 'SVM']\n",
    "\n",
    "directory_full = '../../results/ars-selection/Full/ars_1'\n",
    "# Metrics tables\n",
    "index_labels = ['negative-f', 'insignificant-f','positive-f','negative-u', 'insignificant-u','positive-u','negative-c', 'insignificant-c','positive-c', 'insignificant-f-u and positive-c' ]\n",
    "# Initialize influence table\n",
    "cols = ['ratio', 'system', 'time', 'acc', 'F1_score', 'Precision', 'Recall', 'SPD_gender', 'EOD_gender', 'AOD_gender', 'DI_gender', 'DcI_gender']\n",
    "compute_influence = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "# Initialize full result and cost DataFrames\n",
    "df_full_result = pd.DataFrame()\n",
    "df_full_cost = pd.DataFrame()\n",
    "df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "files = []\n",
    "\n",
    "df_average_results = pd.DataFrame()\n",
    "\n",
    "# Load full system results for comparisons\n",
    "for model in models: \n",
    "\n",
    "    df_full_result = pd.DataFrame()\n",
    "    df_full_cost = pd.DataFrame()\n",
    "    df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if \"fair_metrics_\"+ model in filename :\n",
    "           file_path = os.path.join(directory_full, filename)\n",
    "           if model == 'SVM':\n",
    "               df_r = pd.read_csv(file_path).iloc[34:69, 1:]\n",
    "           else:\n",
    "               df_r = pd.read_csv(file_path).iloc[54:69, 1:]\n",
    "           df_full_result = pd.concat([df_full_result, df_r.abs()], ignore_index=True)\n",
    "\n",
    "        if \"cost_metrics_\" + model in filename and filename.endswith(\"_0.csv\"):\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            if model == 'SVM':\n",
    "                df_ct = pd.read_csv(file_path).iloc[34:69, 1:]  \n",
    "            else:\n",
    "                df_ct = pd.read_csv(file_path).iloc[54:69, 1:]  \n",
    "            df_full_cost_time = pd.concat([df_full_cost_time, df_ct.abs()], ignore_index=True)\n",
    "\n",
    "        if \"cost_metrics_\" + model in filename :\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            if model == 'SVM':\n",
    "                df_c = pd.read_csv(file_path).iloc[34:69, 1:]\n",
    "            else:\n",
    "                df_c = pd.read_csv(file_path).iloc[54:69, 1:]\n",
    "            df_full_cost = pd.concat([df_full_cost, df_c.abs()], ignore_index=True)\n",
    "\n",
    "    print('**********************')\n",
    "    print(model)\n",
    "\n",
    "\n",
    "    for col in ['DI_gender']:\n",
    "        df_full_result[col] = df_full_result[col].apply(lambda x: abs(1 - x))\n",
    "    \n",
    "\n",
    "    ### Calcul de valeurs moyenne pour le full \n",
    "    result_avg = df_full_result.mean()\n",
    "    cost_avg = df_full_cost.mean()\n",
    "    cost_time_avg = df_full_cost_time.mean()\n",
    "\n",
    "    # Ligne de valeurs moyennes de full \n",
    "    avg_data = pd.DataFrame({\n",
    "        **{col + '_avg': [result_avg[col]] for col in result_avg.index},\n",
    "        **{col + '_avg': [cost_avg[col]] for col in cost_avg.index},\n",
    "        **{col + '_avg': [cost_time_avg[col]] for col in cost_time_avg.index},\n",
    "        'dataset': ['ars'],\n",
    "        'model': [model],\n",
    "        'system': ['Full'],\n",
    "        'ratio': [1],\n",
    "    })\n",
    "\n",
    "    df_average_results = pd.concat([df_average_results, avg_data], ignore_index=True)\n",
    "\n",
    "    # Lire les fichiers de système de selection par système et par ratio\n",
    "\n",
    "    for directory_path_1 in systems_path:\n",
    "       for directory_path_2 in ratio_path:\n",
    "           directory = directory_path_1 + directory_path_2\n",
    "           system = directory_path_1.split('/')[-1]           \n",
    "        \n",
    "           df_full_result_selection = pd.DataFrame()\n",
    "           df_full_cost_selection = pd.DataFrame()\n",
    "           df_full_cost_time_selection = pd.DataFrame()\n",
    "        \n",
    "           # Cost file processing\n",
    "           for filename in os.listdir(directory):\n",
    "                if \"fair_metrics_\" + model in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'SVM':\n",
    "                        df_to_add_f = pd.read_csv(file_path).iloc[14:49, 1:].abs()\n",
    "                    else:\n",
    "                        df_to_add_f = pd.read_csv(file_path).iloc[34:49, 1:].abs()\n",
    "                    df_full_result_selection = pd.concat([df_full_result_selection, df_to_add_f], ignore_index=True)\n",
    "                    \n",
    "                if ((\"cost_metrics_\" + model in filename) and (filename.endswith(\"_0.csv\"))):\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'SVM':\n",
    "                        df_to_add_t = pd.read_csv(file_path).iloc[14:49, 1:].abs()\n",
    "                    else:\n",
    "                        df_to_add_t = pd.read_csv(file_path).iloc[34:49, 1:].abs()\n",
    "\n",
    "                    df_full_cost_time_selection = pd.concat([df_full_cost_time_selection, df_to_add_t], ignore_index=True)\n",
    "        \n",
    "                if \"cost_metrics_\" + model in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'SVM':\n",
    "                        df_to_add_c = pd.read_csv(file_path).iloc[14:49, 1:].abs()\n",
    "                    else:\n",
    "                        df_to_add_c = pd.read_csv(file_path).iloc[34:49, 1:].abs()\n",
    "                    df_full_cost_selection = pd.concat([df_full_cost_selection, df_to_add_c], ignore_index=True)\n",
    "           \n",
    "                \n",
    "           for col in ['DI_gender']:\n",
    "             df_full_result_selection[col] = df_full_result_selection[col].apply(lambda x: abs(1 - x))\n",
    "           \n",
    "           ### Calcul de valeurs moyenne pour le full \n",
    "           result_sel_avg = df_full_result_selection.mean()\n",
    "           cost_sel_avg = df_full_cost_selection.mean()\n",
    "           cost_time_sel_avg = df_full_cost_time_selection.mean()\n",
    "\n",
    "           ## Faire les t-test\n",
    "           # Cost\n",
    "           # time\n",
    "           test_time = evaluate_score(df_full_cost_time_selection['Full_training_time'].to_numpy(), df_full_cost_time['Full_training_time'].to_numpy()) \n",
    "           \n",
    "           # Model quality\n",
    "           # accuracy\n",
    "           test_acc = evaluate_score(df_full_cost['accuracy'].to_numpy(), df_full_cost_selection['accuracy'].to_numpy()) \n",
    "           # F1-score\n",
    "           test_f1 = evaluate_score(df_full_result['F1_score'].to_numpy(), df_full_result_selection['F1_score'].to_numpy()) \n",
    "           #precision\n",
    "           test_precision = evaluate_score(df_full_result['Precision'].to_numpy(), df_full_result_selection['Precision'].to_numpy()) \n",
    "           # recall \n",
    "           test_recall = evaluate_score(df_full_result['Recall'].to_numpy(), df_full_result_selection['Recall'].to_numpy()) \n",
    "\n",
    "           # Fairness\n",
    "           # SPD gender\n",
    "           test_spd_g = evaluate_score(df_full_result_selection['SPD_gender'].to_numpy(), df_full_result['SPD_gender'].to_numpy())\n",
    "           # EOD gender\n",
    "           test_eod_g = evaluate_score(df_full_result_selection['EOD_gender'].to_numpy(), df_full_result['EOD_gender'].to_numpy())\n",
    "           # AOD gender\n",
    "           test_aod_g = evaluate_score(df_full_result_selection['AOD_gender'].to_numpy(), df_full_result['AOD_gender'].to_numpy())\n",
    "           # DI gender\n",
    "           test_di_g = evaluate_score(df_full_result_selection['DI_gender'].to_numpy(), df_full_result['DI_gender'].to_numpy())\n",
    "           # DcI gender\n",
    "           test_dci_g = evaluate_score(df_full_result_selection['DcI_gender'].to_numpy(), df_full_result['DcI_gender'].to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "           avg_sel_data = pd.DataFrame({\n",
    "            **{col + '_avg': [result_sel_avg[col]] for col in result_sel_avg.index},\n",
    "            **{col + '_avg': [cost_sel_avg[col]] for col in cost_sel_avg.index},\n",
    "            **{col + '_avg': [cost_time_sel_avg[col]] for col in cost_time_sel_avg.index},\n",
    "           'dataset': ['ars'],\n",
    "           'model': [model],\n",
    "           'system': [system],\n",
    "           'ratio': [directory_path_2.split('_')[-1]],\n",
    "           'test_time' : [test_time], \n",
    "           'test_acc' : [test_acc], \n",
    "           'test_f1' : [test_f1], \n",
    "           'test_precision' : [test_precision], \n",
    "           'test_recall' : [test_recall],\n",
    "           'test_SPD_gender' : [test_spd_g], \n",
    "           'test_EOD_gender' : [test_eod_g], \n",
    "           'test_AOD_gender' : [test_aod_g], \n",
    "           'test_DI_gender' : [test_di_g], \n",
    "           'test_DcI_gender' : [test_dci_g], \n",
    "\n",
    "           'test_SPD_age' : ' ',\n",
    "           'test_EOD_age' : ' ',\n",
    "           'test_AOD_age' : ' ', \n",
    "           'test_DI_age' : ' ', \n",
    "           'test_DcI_age' : ' ', \n",
    "\n",
    "           'test_SPD_race' : ' ',\n",
    "           'test_EOD_race' : ' ',\n",
    "           'test_AOD_race' : ' ', \n",
    "           'test_DI_race' : ' ', \n",
    "           'test_DcI_race' : ' ', \n",
    "            })\n",
    "           \n",
    "           df_average_results = pd.concat([df_average_results, avg_sel_data], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "df_average_results.to_csv('../../results/test/ars_ttest_results_epochs_intervals.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Define paths and initialize variables\n",
    "systems_path = ['../results/adult-selection/CRAIGPB', '../results/adult-selection/GLISTERPB', '../results/adult-selection/GradMatchPB', '../results/adult-selection/Random']\n",
    "ratio_path = ['/adult_0.05', '/adult_0.1', '/adult_0.2','/adult_0.3', '/adult_0.5']\n",
    "models = ['Logreg', 'MLP', 'SVM']  \n",
    "\n",
    "directory_full = '../results/adult-selection/Full/adult_1'\n",
    "\n",
    "\n",
    "# Initialize full result and cost DataFrames\n",
    "df_full_result = pd.DataFrame()\n",
    "df_full_cost = pd.DataFrame()\n",
    "df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "files = []\n",
    "\n",
    "df_average_results = pd.DataFrame()\n",
    "\n",
    "# Load full system results for comparisons\n",
    "for model in models: \n",
    "\n",
    "    df_full_result = pd.DataFrame()\n",
    "    df_full_cost = pd.DataFrame()\n",
    "    df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if \"fair_metrics_\"+ model in filename :\n",
    "           file_path = os.path.join(directory_full, filename)\n",
    "           if model == 'MLP':\n",
    "             df_r = pd.read_csv(file_path).iloc[89:199, 1:]\n",
    "           else :\n",
    "             df_r = pd.read_csv(file_path).iloc[54:99, 1:]\n",
    "           df_full_result = pd.concat([df_full_result, df_r.abs()], ignore_index=True)\n",
    "\n",
    "        if \"cost_metrics_\" + model in filename and filename.endswith(\"_0.csv\"):\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            if model == 'MLP':\n",
    "             df_ct = pd.read_csv(file_path).iloc[89:199, 1:]\n",
    "            else :\n",
    "             df_ct = pd.read_csv(file_path).iloc[54:99, 1:]\n",
    "            df_full_cost_time = pd.concat([df_full_cost_time, df_ct.abs()], ignore_index=True)\n",
    "\n",
    "        if \"cost_metrics_\" + model in filename :\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            if model == 'MLP':\n",
    "             df_c = pd.read_csv(file_path).iloc[89:199, 1:]\n",
    "            else :\n",
    "             df_c = pd.read_csv(file_path).iloc[54:99, 1:]\n",
    "            df_full_cost = pd.concat([df_full_cost, df_c.abs()], ignore_index=True)\n",
    "\n",
    "    print('**********************')\n",
    "    print(model)\n",
    "\n",
    "    for col in ['DI_gender', 'DI_age', 'DI_race']:\n",
    "        df_full_result[col] = df_full_result[col].apply(lambda x: abs(1 - x))\n",
    "    \n",
    "\n",
    "    ### Calcul de valeurs moyenne pour le full \n",
    "    result_avg = df_full_result.mean()\n",
    "    cost_avg = df_full_cost.mean()\n",
    "    cost_time_avg = df_full_cost_time.mean()\n",
    "\n",
    "    # Ligne de valeurs moyennes de full \n",
    "    avg_data = pd.DataFrame({\n",
    "        **{col + '_avg': [result_avg[col]] for col in result_avg.index},\n",
    "        **{col + '_avg': [cost_avg[col]] for col in cost_avg.index},\n",
    "        **{col + '_avg': [cost_time_avg[col]] for col in cost_time_avg.index},\n",
    "        'dataset': ['adult'],\n",
    "        'model': [model],\n",
    "        'system': ['Full'],\n",
    "        'ratio': [1],\n",
    "    })\n",
    "\n",
    "    df_average_results = pd.concat([df_average_results, avg_data], ignore_index=True)\n",
    "\n",
    "    # Lire les fichiers de système de selection par système et par ratio\n",
    "\n",
    "    for directory_path_1 in systems_path:\n",
    "       for directory_path_2 in ratio_path:\n",
    "           directory = directory_path_1 + directory_path_2\n",
    "           system = directory_path_1.split('/')[-1]           \n",
    "        \n",
    "           df_full_result_selection = pd.DataFrame()\n",
    "           df_full_cost_selection = pd.DataFrame()\n",
    "           df_full_cost_time_selection = pd.DataFrame()\n",
    "        \n",
    "           # Cost file processing\n",
    "           for filename in os.listdir(directory):\n",
    "                if \"fair_metrics_\" + model in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP':\n",
    "                       df_to_add_f = pd.read_csv(file_path).iloc[69:179, 1:].abs()\n",
    "                       \n",
    "                    else:\n",
    "                       df_to_add_f = pd.read_csv(file_path).iloc[34:79, 1:].abs()\n",
    "                    df_full_result_selection = pd.concat([df_full_result_selection, df_to_add_f], ignore_index=True)\n",
    "                    \n",
    "\n",
    "                if ((\"cost_metrics_\" + model in filename) and (filename.endswith(\"_0.csv\"))):\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP':\n",
    "                       df_to_add_t = pd.read_csv(file_path).iloc[69:179, 1:].abs()\n",
    "                    else:\n",
    "                       df_to_add_t = pd.read_csv(file_path).iloc[34:79, 1:].abs()\n",
    "                    df_full_cost_time_selection = pd.concat([df_full_cost_time_selection, df_to_add_t], ignore_index=True)\n",
    "    \n",
    "\n",
    "        \n",
    "                if \"cost_metrics_\" + model in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP':\n",
    "                       df_to_add_c = pd.read_csv(file_path).iloc[69:179, 1:].abs()\n",
    "                    else:\n",
    "                       df_to_add_c = pd.read_csv(file_path).iloc[34:79, 1:].abs()\n",
    "                    df_full_cost_selection = pd.concat([df_full_cost_selection, df_to_add_c], ignore_index=True)\n",
    "           \n",
    "                \n",
    "           for col in ['DI_gender', 'DI_age', 'DI_race']:\n",
    "             df_full_result_selection[col] = df_full_result_selection[col].apply(lambda x: abs(1 - x))\n",
    "           \n",
    "           ### Calcul de valeurs moyenne pour le full \n",
    "           result_sel_avg = df_full_result_selection.mean()\n",
    "           cost_sel_avg = df_full_cost_selection.mean()\n",
    "           cost_time_sel_avg = df_full_cost_time_selection.mean()\n",
    "\n",
    "           ## Faire les t-test\n",
    "           # Cost\n",
    "           # time\n",
    "           test_time = evaluate_score(df_full_cost_time_selection['Full_training_time'].to_numpy(), df_full_cost_time['Full_training_time'].to_numpy()) \n",
    "           \n",
    "           # Model quality\n",
    "           # accuracy\n",
    "           test_acc = evaluate_score(df_full_cost['accuracy'].to_numpy(), df_full_cost_selection['accuracy'].to_numpy()) \n",
    "           # F1-score\n",
    "           test_f1 = evaluate_score(df_full_result['F1_score'].to_numpy(), df_full_result_selection['F1_score'].to_numpy()) \n",
    "           #precision\n",
    "           test_precision = evaluate_score(df_full_result['Precision'].to_numpy(), df_full_result_selection['Precision'].to_numpy()) \n",
    "           # recall \n",
    "           test_recall = evaluate_score(df_full_result['Recall'].to_numpy(), df_full_result_selection['Recall'].to_numpy()) \n",
    "\n",
    "           # Fairness\n",
    "           # SPD gender\n",
    "           test_spd_g = evaluate_score(df_full_result_selection['SPD_gender'].to_numpy(), df_full_result['SPD_gender'].to_numpy())\n",
    "           # EOD gender\n",
    "           test_eod_g = evaluate_score(df_full_result_selection['EOD_gender'].to_numpy(), df_full_result['EOD_gender'].to_numpy())\n",
    "           # AOD gender\n",
    "           test_aod_g = evaluate_score(df_full_result_selection['AOD_gender'].to_numpy(), df_full_result['AOD_gender'].to_numpy())\n",
    "           # DI gender\n",
    "           test_di_g = evaluate_score(df_full_result_selection['DI_gender'].to_numpy(), df_full_result['DI_gender'].to_numpy())\n",
    "           # DcI gender\n",
    "           test_dci_g = evaluate_score(df_full_result_selection['DcI_gender'].to_numpy(), df_full_result['DcI_gender'].to_numpy())\n",
    "\n",
    "           # SPD age\n",
    "           test_spd_a = evaluate_score(df_full_result_selection['SPD_age'].to_numpy(), df_full_result['SPD_age'].to_numpy())\n",
    "           # EOD gender\n",
    "           test_eod_a = evaluate_score(df_full_result_selection['EOD_age'].to_numpy(), df_full_result['EOD_age'].to_numpy())\n",
    "           # AOD gender\n",
    "           test_aod_a = evaluate_score(df_full_result_selection['AOD_age'].to_numpy(), df_full_result['AOD_age'].to_numpy())\n",
    "           # DI gender\n",
    "           test_di_a = evaluate_score(df_full_result_selection['DI_age'].to_numpy(), df_full_result['DI_age'].to_numpy())\n",
    "           # DcI gender\n",
    "           test_dci_a = evaluate_score(df_full_result_selection['DcI_age'].to_numpy(), df_full_result['DcI_age'].to_numpy())\n",
    "\n",
    "           # SPD race\n",
    "           test_spd_r = evaluate_score(df_full_result_selection['SPD_race'].to_numpy(), df_full_result['SPD_race'].to_numpy())\n",
    "           # EOD race\n",
    "           test_eod_r = evaluate_score(df_full_result_selection['EOD_race'].to_numpy(), df_full_result['EOD_race'].to_numpy())\n",
    "           # AOD race\n",
    "           test_aod_r = evaluate_score(df_full_result_selection['AOD_race'].to_numpy(), df_full_result['AOD_race'].to_numpy())\n",
    "           # DI race\n",
    "           test_di_r = evaluate_score(df_full_result_selection['DI_race'].to_numpy(), df_full_result['DI_race'].to_numpy())\n",
    "           # DcI race\n",
    "           test_dci_r = evaluate_score(df_full_result_selection['DcI_race'].to_numpy(), df_full_result['DcI_race'].to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "           avg_sel_data = pd.DataFrame({\n",
    "            **{col + '_avg': [result_sel_avg[col]] for col in result_sel_avg.index},\n",
    "            **{col + '_avg': [cost_sel_avg[col]] for col in cost_sel_avg.index},\n",
    "            **{col + '_avg': [cost_time_sel_avg[col]] for col in cost_time_sel_avg.index},\n",
    "           'dataset': ['adult'],\n",
    "           'model': [model],\n",
    "           'system': [system],\n",
    "           'ratio': [directory_path_2.split('_')[-1]],\n",
    "           'test_time' : [test_time], \n",
    "           'test_acc' : [test_acc], \n",
    "           'test_f1' : [test_f1], \n",
    "           'test_precision' : [test_precision], \n",
    "           'test_recall' : [test_recall],\n",
    "           'test_SPD_gender' : [test_spd_g], \n",
    "           'test_EOD_gender' : [test_eod_g], \n",
    "           'test_AOD_gender' : [test_aod_g], \n",
    "           'test_DI_gender' : [test_di_g], \n",
    "           'test_DcI_gender' : [test_dci_g], \n",
    "\n",
    "           'test_SPD_age' : [test_spd_a],\n",
    "           'test_EOD_age' : [test_eod_a],\n",
    "           'test_AOD_age' : [test_aod_a], \n",
    "           'test_DI_age' : [test_di_a], \n",
    "           'test_DcI_age' : [test_dci_a], \n",
    "\n",
    "           'test_SPD_race' : [test_spd_r],\n",
    "           'test_EOD_race' : [test_eod_r],\n",
    "           'test_AOD_race' : [test_aod_r], \n",
    "           'test_DI_race' : [test_di_r],\n",
    "           'test_DcI_race' : [test_dci_r],\n",
    "            })\n",
    "           \n",
    "           df_average_results = pd.concat([df_average_results, avg_sel_data], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "df_average_results.to_csv('../results/test/adult_ttest_results_epochs_intervals.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KDD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Define paths and initialize variables\n",
    "systems_path = ['../results/kdd-selection/CRAIGPB', '../results/kdd-selection/GLISTERPB', '../results/kdd-selection/GradMatchPB', '../results/kdd-selection/Random']\n",
    "ratio_path = ['/kdd_0.05', '/kdd_0.1', '/kdd_0.2','/kdd_0.3', '/kdd_0.5']\n",
    "models = ['Logreg', 'MLP', 'SVM']  \n",
    "\n",
    "directory_full = '../results/kdd-selection/Full/kdd_1'\n",
    "\n",
    "\n",
    "# Initialize full result and cost DataFrames\n",
    "df_full_result = pd.DataFrame()\n",
    "df_full_cost = pd.DataFrame()\n",
    "df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "files = []\n",
    "\n",
    "df_average_results = pd.DataFrame()\n",
    "\n",
    "# Load full system results for comparisons\n",
    "for model in models: \n",
    "\n",
    "    df_full_result = pd.DataFrame()\n",
    "    df_full_cost = pd.DataFrame()\n",
    "    df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if \"fair_metrics_\"+ model in filename :\n",
    "           file_path = os.path.join(directory_full, filename)\n",
    "           if model == 'MLP':\n",
    "              df_r = pd.read_csv(file_path).iloc[34:79, 1:]\n",
    "           else:\n",
    "              df_r = pd.read_csv(file_path).iloc[24:79, 1:]\n",
    "           df_full_result = pd.concat([df_full_result, df_r.abs()], ignore_index=True)\n",
    "\n",
    "        if \"cost_metrics_\" + model in filename and filename.endswith(\"_0.csv\"):\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            if model == 'MLP':\n",
    "               df_ct = pd.read_csv(file_path).iloc[34:79, 1:]\n",
    "            else:\n",
    "               df_ct = pd.read_csv(file_path).iloc[24:79, 1:]\n",
    "            df_full_cost_time = pd.concat([df_full_cost_time, df_ct.abs()], ignore_index=True)\n",
    "\n",
    "        if \"cost_metrics_\" + model in filename :\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            if model == 'MLP':\n",
    "               df_c = pd.read_csv(file_path).iloc[34:79, 1:]\n",
    "            else:\n",
    "               df_c = pd.read_csv(file_path).iloc[24:79, 1:]\n",
    "            df_full_cost = pd.concat([df_full_cost, df_c.abs()], ignore_index=True)\n",
    "\n",
    "    print('**********************')\n",
    "    print(model)\n",
    "\n",
    "    for col in ['DI_gender', 'DI_age', 'DI_race']:\n",
    "      df_full_result[col] = df_full_result[col].apply(lambda x: abs(1 - x))\n",
    "    \n",
    "\n",
    "    ### Calcul de valeurs moyenne pour le full \n",
    "    result_avg = df_full_result.mean()\n",
    "    cost_avg = df_full_cost.mean()\n",
    "    cost_time_avg = df_full_cost_time.mean()\n",
    "\n",
    "    # Ligne de valeurs moyennes de full \n",
    "    avg_data = pd.DataFrame({\n",
    "        **{col + '_avg': [result_avg[col]] for col in result_avg.index},\n",
    "        **{col + '_avg': [cost_avg[col]] for col in cost_avg.index},\n",
    "        **{col + '_avg': [cost_time_avg[col]] for col in cost_time_avg.index},\n",
    "        'dataset': ['kdd'],\n",
    "        'model': [model],\n",
    "        'system': ['Full'],\n",
    "        'ratio': [1],\n",
    "    })\n",
    "\n",
    "    df_average_results = pd.concat([df_average_results, avg_data], ignore_index=True)\n",
    "\n",
    "    # Lire les fichiers de système de selection par système et par ratio\n",
    "\n",
    "    for directory_path_1 in systems_path:\n",
    "       for directory_path_2 in ratio_path:\n",
    "           directory = directory_path_1 + directory_path_2\n",
    "           system = directory_path_1.split('/')[-1]           \n",
    "        \n",
    "           df_full_result_selection = pd.DataFrame()\n",
    "           df_full_cost_selection = pd.DataFrame()\n",
    "           df_full_cost_time_selection = pd.DataFrame()\n",
    "        \n",
    "           # Cost file processing\n",
    "           for filename in os.listdir(directory):\n",
    "                if \"fair_metrics_\" + model in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP':\n",
    "                       df_to_add_f = pd.read_csv(file_path).iloc[14:59, 1:].abs()\n",
    "                    else:\n",
    "                       df_to_add_f = pd.read_csv(file_path).iloc[4:59, 1:].abs()\n",
    "                    df_full_result_selection = pd.concat([df_full_result_selection, df_to_add_f], ignore_index=True)\n",
    "                    \n",
    "\n",
    "                if ((\"cost_metrics_\" + model in filename) and (filename.endswith(\"_0.csv\"))):\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP':\n",
    "                       df_to_add_t = pd.read_csv(file_path).iloc[14:59, 1:].abs()\n",
    "                    else:\n",
    "                       df_to_add_t = pd.read_csv(file_path).iloc[4:59, 1:].abs()\n",
    "                    df_full_cost_time_selection = pd.concat([df_full_cost_time_selection, df_to_add_t], ignore_index=True)\n",
    "    \n",
    "\n",
    "        \n",
    "                if \"cost_metrics_\" + model in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP':\n",
    "                       df_to_add_c = pd.read_csv(file_path).iloc[14:59, 1:].abs()\n",
    "                    else:\n",
    "                       df_to_add_c = pd.read_csv(file_path).iloc[4:59, 1:].abs()\n",
    "                    df_full_cost_selection = pd.concat([df_full_cost_selection, df_to_add_c], ignore_index=True)\n",
    "           \n",
    "                \n",
    "           for col in ['DI_gender', 'DI_age', 'DI_race']:\n",
    "             df_full_result_selection[col] = df_full_result_selection[col].apply(lambda x: abs(1 - x))\n",
    "\n",
    "           ### Calcul de valeurs moyenne pour le full \n",
    "           result_sel_avg = df_full_result_selection.mean()\n",
    "           cost_sel_avg = df_full_cost_selection.mean()\n",
    "           cost_time_sel_avg = df_full_cost_time_selection.mean()\n",
    "\n",
    "           ## Faire les t-test\n",
    "           # Cost\n",
    "           # time\n",
    "           test_time = evaluate_score(df_full_cost_time_selection['Full_training_time'].to_numpy(), df_full_cost_time['Full_training_time'].to_numpy()) \n",
    "           \n",
    "           # Model quality\n",
    "           # accuracy\n",
    "           test_acc = evaluate_score(df_full_cost['accuracy'].to_numpy(), df_full_cost_selection['accuracy'].to_numpy()) \n",
    "           # F1-score\n",
    "           test_f1 = evaluate_score(df_full_result['F1_score'].to_numpy(), df_full_result_selection['F1_score'].to_numpy()) \n",
    "           #precision\n",
    "           test_precision = evaluate_score(df_full_result['Precision'].to_numpy(), df_full_result_selection['Precision'].to_numpy()) \n",
    "           # recall \n",
    "           test_recall = evaluate_score(df_full_result['Recall'].to_numpy(), df_full_result_selection['Recall'].to_numpy()) \n",
    "\n",
    "           # Fairness\n",
    "           # SPD gender\n",
    "           test_spd_g = evaluate_score(df_full_result_selection['SPD_gender'].to_numpy(), df_full_result['SPD_gender'].to_numpy())\n",
    "           # EOD gender\n",
    "           test_eod_g = evaluate_score(df_full_result_selection['EOD_gender'].to_numpy(), df_full_result['EOD_gender'].to_numpy())\n",
    "           # AOD gender\n",
    "           test_aod_g = evaluate_score(df_full_result_selection['AOD_gender'].to_numpy(), df_full_result['AOD_gender'].to_numpy())\n",
    "           # DI gender\n",
    "           test_di_g = evaluate_score(df_full_result_selection['DI_gender'].to_numpy(), df_full_result['DI_gender'].to_numpy())\n",
    "           # DcI gender\n",
    "           test_dci_g = evaluate_score(df_full_result_selection['DcI_gender'].to_numpy(), df_full_result['DcI_gender'].to_numpy())\n",
    "\n",
    "           # SPD age\n",
    "           test_spd_a = evaluate_score(df_full_result_selection['SPD_age'].to_numpy(), df_full_result['SPD_age'].to_numpy())\n",
    "           # EOD gender\n",
    "           test_eod_a = evaluate_score(df_full_result_selection['EOD_age'].to_numpy(), df_full_result['EOD_age'].to_numpy())\n",
    "           # AOD gender\n",
    "           test_aod_a = evaluate_score(df_full_result_selection['AOD_age'].to_numpy(), df_full_result['AOD_age'].to_numpy())\n",
    "           # DI gender\n",
    "           test_di_a = evaluate_score(df_full_result_selection['DI_age'].to_numpy(), df_full_result['DI_age'].to_numpy())\n",
    "           # DcI gender\n",
    "           test_dci_a = evaluate_score(df_full_result_selection['DcI_age'].to_numpy(), df_full_result['DcI_age'].to_numpy())\n",
    "\n",
    "           # SPD race\n",
    "           test_spd_r = evaluate_score(df_full_result_selection['SPD_race'].to_numpy(), df_full_result['SPD_race'].to_numpy())\n",
    "           # EOD race\n",
    "           test_eod_r = evaluate_score(df_full_result_selection['EOD_race'].to_numpy(), df_full_result['EOD_race'].to_numpy())\n",
    "           # AOD race\n",
    "           test_aod_r = evaluate_score(df_full_result_selection['AOD_race'].to_numpy(), df_full_result['AOD_race'].to_numpy())\n",
    "           # DI race\n",
    "           test_di_r = evaluate_score(df_full_result_selection['DI_race'].to_numpy(), df_full_result['DI_race'].to_numpy())\n",
    "           # DcI race\n",
    "           test_dci_r = evaluate_score(df_full_result_selection['DcI_race'].to_numpy(), df_full_result['DcI_race'].to_numpy())\n",
    "\n",
    "\n",
    "\n",
    "           avg_sel_data = pd.DataFrame({\n",
    "            **{col + '_avg': [result_sel_avg[col]] for col in result_sel_avg.index},\n",
    "            **{col + '_avg': [cost_sel_avg[col]] for col in cost_sel_avg.index},\n",
    "            **{col + '_avg': [cost_time_sel_avg[col]] for col in cost_time_sel_avg.index},\n",
    "           'dataset': ['kdd'],\n",
    "           'model': [model],\n",
    "           'system': [system],\n",
    "           'ratio': [directory_path_2.split('_')[-1]],\n",
    "           'test_time' : [test_time], \n",
    "           'test_acc' : [test_acc], \n",
    "           'test_f1' : [test_f1], \n",
    "           'test_precision' : [test_precision], \n",
    "           'test_recall' : [test_recall],\n",
    "           'test_SPD_gender' : [test_spd_g], \n",
    "           'test_EOD_gender' : [test_eod_g], \n",
    "           'test_AOD_gender' : [test_aod_g], \n",
    "           'test_DI_gender' : [test_di_g], \n",
    "           'test_DcI_gender' : [test_dci_g], \n",
    "\n",
    "           'test_SPD_age' : [test_spd_a],\n",
    "           'test_EOD_age' : [test_eod_a],\n",
    "           'test_AOD_age' : [test_aod_a], \n",
    "           'test_DI_age' : [test_di_a], \n",
    "           'test_DcI_age' : [test_dci_a], \n",
    "\n",
    "           'test_SPD_race' : [test_spd_r],\n",
    "           'test_EOD_race' : [test_eod_r],\n",
    "           'test_AOD_race' : [test_aod_r], \n",
    "           'test_DI_race' : [test_di_r],\n",
    "           'test_DcI_race' : [test_dci_r],\n",
    "            })\n",
    "           \n",
    "           df_average_results = pd.concat([df_average_results, avg_sel_data], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "df_average_results.to_csv('../results/test/kdd_ttest_results_epochs_intervals.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Define paths and initialize variables\n",
    "systems_path = ['../results/dc-selection/CRAIGPB', '../results/dc-selection/GLISTERPB', '../results/dc-selection/GradMatchPB', '../results/dc-selection/Random']\n",
    "ratio_path = ['/dc_0.05', '/dc_0.1', '/dc_0.2','/dc_0.3', '/dc_0.5']\n",
    "models = ['LogReg', 'MLP', 'SVM']  \n",
    "\n",
    "directory_full = '../results/dc-selection/Full/dc_1'\n",
    "\n",
    "\n",
    "# Initialize full result and cost DataFrames\n",
    "df_full_result = pd.DataFrame()\n",
    "df_full_cost = pd.DataFrame()\n",
    "df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "files = []\n",
    "\n",
    "df_average_results = pd.DataFrame()\n",
    "\n",
    "# Load full system results for comparisons\n",
    "for model in models: \n",
    "\n",
    "    df_full_result = pd.DataFrame()\n",
    "    df_full_cost = pd.DataFrame()\n",
    "    df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if \"fair_metrics_\"+ model in filename :\n",
    "           file_path = os.path.join(directory_full, filename)\n",
    "           if model == 'MLP':\n",
    "               df_r = pd.read_csv(file_path).iloc[44:99, 1:]\n",
    "           elif model == 'SVM':\n",
    "               df_r = pd.read_csv(file_path).iloc[29:99, 1:]\n",
    "           else:\n",
    "               df_r = pd.read_csv(file_path).iloc[124:269, 1:]\n",
    "           df_full_result = pd.concat([df_full_result, df_r.abs()], ignore_index=True)\n",
    "\n",
    "        if \"cost_metrics_\" + model in filename and filename.endswith(\"_0.csv\"):\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            if model == 'MLP':\n",
    "               df_ct = pd.read_csv(file_path).iloc[44:99, 1:]\n",
    "            elif model == 'SVM':\n",
    "               df_ct = pd.read_csv(file_path).iloc[29:99, 1:]\n",
    "            else:\n",
    "               df_ct = pd.read_csv(file_path).iloc[124:269, 1:]\n",
    "            df_full_cost_time = pd.concat([df_full_cost_time, df_ct.abs()], ignore_index=True)\n",
    "\n",
    "        if \"cost_metrics_\" + model in filename :\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            if model == 'MLP':\n",
    "               df_c = pd.read_csv(file_path).iloc[44:99, 1:]\n",
    "            elif model == 'SVM':\n",
    "               df_c = pd.read_csv(file_path).iloc[29:99, 1:]\n",
    "            else:\n",
    "               df_c = pd.read_csv(file_path).iloc[124:269, 1:]\n",
    "            df_full_cost = pd.concat([df_full_cost, df_c.abs()], ignore_index=True)\n",
    "\n",
    "    print('**********************')\n",
    "    print(model)\n",
    "\n",
    "    for col in ['DI_gender', 'DI_age']:\n",
    "      df_full_result[col] = df_full_result[col].apply(lambda x: abs(1 - x))\n",
    "    \n",
    "\n",
    "    ### Calcul de valeurs moyenne pour le full \n",
    "    result_avg = df_full_result.mean()\n",
    "    cost_avg = df_full_cost.mean()\n",
    "    cost_time_avg = df_full_cost_time.mean()\n",
    "\n",
    "    # Ligne de valeurs moyennes de full \n",
    "    avg_data = pd.DataFrame({\n",
    "        **{col + '_avg': [result_avg[col]] for col in result_avg.index},\n",
    "        **{col + '_avg': [cost_avg[col]] for col in cost_avg.index},\n",
    "        **{col + '_avg': [cost_time_avg[col]] for col in cost_time_avg.index},\n",
    "        'dataset': ['dc'],\n",
    "        'model': [model],\n",
    "        'system': ['Full'],\n",
    "        'ratio': [1],\n",
    "    })\n",
    "\n",
    "    df_average_results = pd.concat([df_average_results, avg_data], ignore_index=True)\n",
    "\n",
    "    # Lire les fichiers de système de selection par système et par ratio\n",
    "\n",
    "    for directory_path_1 in systems_path:\n",
    "       for directory_path_2 in ratio_path:\n",
    "           directory = directory_path_1 + directory_path_2\n",
    "           system = directory_path_1.split('/')[-1]           \n",
    "        \n",
    "           df_full_result_selection = pd.DataFrame()\n",
    "           df_full_cost_selection = pd.DataFrame()\n",
    "           df_full_cost_time_selection = pd.DataFrame()\n",
    "        \n",
    "           \n",
    "           # Cost file processing\n",
    "           for filename in os.listdir(directory):\n",
    "                if \"fair_metrics_\" + model in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP': \n",
    "                        df_to_add_f = pd.read_csv(file_path).iloc[44:99, 1:].abs()\n",
    "                    elif model == 'SVM':\n",
    "                        df_to_add_f = pd.read_csv(file_path).iloc[29:99, 1:].abs()\n",
    "                    else:\n",
    "                        df_to_add_f = pd.read_csv(file_path).iloc[124:269, 1:].abs()\n",
    "                    df_full_result_selection = pd.concat([df_full_result_selection, df_to_add_f], ignore_index=True)\n",
    "                    \n",
    "\n",
    "                if ((\"cost_metrics_\" + model in filename) and (filename.endswith(\"_0.csv\"))):\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP': \n",
    "                        df_to_add_t = pd.read_csv(file_path).iloc[44:99, 1:].abs()\n",
    "                    elif model == 'SVM':\n",
    "                        df_to_add_t = pd.read_csv(file_path).iloc[29:99, 1:].abs()\n",
    "                    else: \n",
    "                        df_to_add_t = pd.read_csv(file_path).iloc[124:269, 1:].abs()\n",
    "                    df_full_cost_time_selection = pd.concat([df_full_cost_time_selection, df_to_add_t], ignore_index=True)\n",
    "    \n",
    "\n",
    "                \n",
    "\n",
    "                if \"cost_metrics_\" + model in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    if model == 'MLP': \n",
    "                        df_to_add_c = pd.read_csv(file_path).iloc[44:99, 1:].abs()\n",
    "                    elif model == 'SVM':\n",
    "                        df_to_add_c = pd.read_csv(file_path).iloc[29:99, 1:].abs()\n",
    "                    else:\n",
    "                        df_to_add_c = pd.read_csv(file_path).iloc[124:269, 1:].abs()\n",
    "                    df_full_cost_selection = pd.concat([df_full_cost_selection, df_to_add_c], ignore_index=True)\n",
    "           \n",
    "                \n",
    "           for col in ['DI_gender', 'DI_age']:\n",
    "             df_full_result_selection[col] = df_full_result_selection[col].apply(lambda x: abs(1 - x))\n",
    "           \n",
    "           ### Calcul de valeurs moyenne pour le full \n",
    "           result_sel_avg = df_full_result_selection.mean()\n",
    "           cost_sel_avg = df_full_cost_selection.mean()\n",
    "           cost_time_sel_avg = df_full_cost_time_selection.mean()\n",
    "\n",
    "           ## Faire les t-test\n",
    "           # Cost\n",
    "           # time\n",
    "           test_time = evaluate_score(df_full_cost_time_selection['Full_training_time'].to_numpy(), df_full_cost_time['Full_training_time'].to_numpy()) \n",
    "           \n",
    "           # Model quality\n",
    "           # accuracy\n",
    "           test_acc = evaluate_score(df_full_cost['accuracy'].to_numpy(), df_full_cost_selection['accuracy'].to_numpy()) \n",
    "           # F1-score\n",
    "           test_f1 = evaluate_score(df_full_result['F1_score'].to_numpy(), df_full_result_selection['F1_score'].to_numpy()) \n",
    "           #precision\n",
    "           test_precision = evaluate_score(df_full_result['Precision'].to_numpy(), df_full_result_selection['Precision'].to_numpy()) \n",
    "           # recall \n",
    "           test_recall = evaluate_score(df_full_result['Recall'].to_numpy(), df_full_result_selection['Recall'].to_numpy()) \n",
    "\n",
    "           # Fairness\n",
    "           # SPD gender\n",
    "           test_spd_g = evaluate_score(df_full_result_selection['SPD_gender'].to_numpy(), df_full_result['SPD_gender'].to_numpy())\n",
    "           # EOD gender\n",
    "           test_eod_g = evaluate_score(df_full_result_selection['EOD_gender'].to_numpy(), df_full_result['EOD_gender'].to_numpy())\n",
    "           # AOD gender\n",
    "           test_aod_g = evaluate_score(df_full_result_selection['AOD_gender'].to_numpy(), df_full_result['AOD_gender'].to_numpy())\n",
    "           # DI gender\n",
    "           test_di_g = evaluate_score(df_full_result_selection['DI_gender'].to_numpy(), df_full_result['DI_gender'].to_numpy())\n",
    "           # DcI gender\n",
    "           test_dci_g = evaluate_score(df_full_result_selection['DcI_gender'].to_numpy(), df_full_result['DcI_gender'].to_numpy())\n",
    "\n",
    "           # SPD age\n",
    "           test_spd_a = evaluate_score(df_full_result_selection['SPD_age'].to_numpy(), df_full_result['SPD_age'].to_numpy())\n",
    "           # EOD age\n",
    "           test_eod_a = evaluate_score(df_full_result_selection['EOD_age'].to_numpy(), df_full_result['EOD_age'].to_numpy())\n",
    "           # AOD age\n",
    "           test_aod_a = evaluate_score(df_full_result_selection['AOD_age'].to_numpy(), df_full_result['AOD_age'].to_numpy())\n",
    "           # DI age\n",
    "           test_di_a = evaluate_score(df_full_result_selection['DI_age'].to_numpy(), df_full_result['DI_age'].to_numpy())\n",
    "           # DcI age\n",
    "           test_dci_a = evaluate_score(df_full_result_selection['DcI_age'].to_numpy(), df_full_result['DcI_age'].to_numpy())\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "           avg_sel_data = pd.DataFrame({\n",
    "            **{col + '_avg': [result_sel_avg[col]] for col in result_sel_avg.index},\n",
    "            **{col + '_avg': [cost_sel_avg[col]] for col in cost_sel_avg.index},\n",
    "            **{col + '_avg': [cost_time_sel_avg[col]] for col in cost_time_sel_avg.index},\n",
    "           'dataset': ['dc'],\n",
    "           'model': [model],\n",
    "           'system': [system],\n",
    "           'ratio': [directory_path_2.split('_')[-1]],\n",
    "           'test_time' : [test_time], \n",
    "           'test_acc' : [test_acc], \n",
    "           'test_f1' : [test_f1], \n",
    "           'test_precision' : [test_precision], \n",
    "           'test_recall' : [test_recall],\n",
    "           'test_SPD_gender' : [test_spd_g], \n",
    "           'test_EOD_gender' : [test_eod_g], \n",
    "           'test_AOD_gender' : [test_aod_g], \n",
    "           'test_DI_gender' : [test_di_g], \n",
    "           'test_DcI_gender' : [test_dci_g], \n",
    "\n",
    "           'test_SPD_age' : [test_spd_a],\n",
    "           'test_EOD_age' : [test_eod_a],\n",
    "           'test_AOD_age' : [test_aod_a], \n",
    "           'test_DI_age' : [test_di_a], \n",
    "           'test_DcI_age' : [test_dci_a], \n",
    "\n",
    "           'test_SPD_race' : ' ',\n",
    "           'test_EOD_race' : ' ',\n",
    "           'test_AOD_race' : ' ', \n",
    "           'test_DI_race' : ' ',\n",
    "           'test_DcI_race' : ' ',\n",
    "            })\n",
    "           \n",
    "           df_average_results = pd.concat([df_average_results, avg_sel_data], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "df_average_results.to_csv('../results/test/dc_ttest_results_epochs_intervals.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MobiAct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Define paths and initialize variables\n",
    "systems_path = ['../results/mobiact-selection/CRAIGPB', '../results/mobiact-selection/GLISTERPB', '../results/mobiact-selection/GradMatchPB', '../results/mobiact-selection/Random']\n",
    "ratio_path = ['/mobiact_0.05', '/mobiact_0.1', '/mobiact_0.2','/mobiact_0.3', '/mobiact_0.5']\n",
    "models = ['MLP']  \n",
    "\n",
    "directory_full = '../results/mobiact-selection/Full/mobiact_1'\n",
    "\n",
    "\n",
    "# Initialize full result and cost DataFrames\n",
    "df_full_result = pd.DataFrame()\n",
    "df_full_cost = pd.DataFrame()\n",
    "df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "i = 0\n",
    "files = []\n",
    "\n",
    "df_average_results = pd.DataFrame()\n",
    "\n",
    "# Load full system results for comparisons\n",
    "for model in models: \n",
    "\n",
    "    df_full_result = pd.DataFrame()\n",
    "    df_full_cost = pd.DataFrame()\n",
    "    df_full_cost_time = pd.DataFrame()\n",
    "\n",
    "    for filename in os.listdir(directory_full):\n",
    "        if \"train_mobiact_fair_metrics_\" in filename :\n",
    "           file_path = os.path.join(directory_full, filename)\n",
    "           df_r = pd.read_csv(file_path).iloc[29:79, 1:]\n",
    "           df_full_result = pd.concat([df_full_result, df_r.abs()], ignore_index=True)\n",
    "\n",
    "        if \"train_mobiact_cost_metrics_\"  in filename and filename.endswith(\"_0.csv\"):\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            df_ct = pd.read_csv(file_path).iloc[29:79, 1:]\n",
    "            df_full_cost_time = pd.concat([df_full_cost_time, df_ct.abs()], ignore_index=True)\n",
    "\n",
    "        if \"train_mobiact_cost_metrics_\"  in filename :\n",
    "            file_path = os.path.join(directory_full, filename)\n",
    "            df_c = pd.read_csv(file_path).iloc[29:79, 1:]\n",
    "            df_full_cost = pd.concat([df_full_cost, df_c.abs()], ignore_index=True)\n",
    "\n",
    "    print('**********************')\n",
    "    print(model)\n",
    "\n",
    "    for col in ['DI_gender', 'DI_age']:\n",
    "      df_full_result[col] = df_full_result[col].apply(lambda x: abs(1 - x))\n",
    "\n",
    "    ### Calcul de valeurs moyenne pour le full \n",
    "    result_avg = df_full_result.mean()\n",
    "    cost_avg = df_full_cost.mean()\n",
    "    cost_time_avg = df_full_cost_time.mean()\n",
    "\n",
    "    # Ligne de valeurs moyennes de full \n",
    "    avg_data = pd.DataFrame({\n",
    "        **{col + '_avg': [result_avg[col]] for col in result_avg.index},\n",
    "        **{col + '_avg': [cost_avg[col]] for col in cost_avg.index},\n",
    "        **{col + '_avg': [cost_time_avg[col]] for col in cost_time_avg.index},\n",
    "        'dataset': ['mobiact'],\n",
    "        'model': [model],\n",
    "        'system': ['Full'],\n",
    "        'ratio': [1],\n",
    "    })\n",
    "\n",
    "    df_average_results = pd.concat([df_average_results, avg_data], ignore_index=True)\n",
    "\n",
    "    # Lire les fichiers de système de selection par système et par ratio\n",
    "\n",
    "    for directory_path_1 in systems_path:\n",
    "       for directory_path_2 in ratio_path:\n",
    "           directory = directory_path_1 + directory_path_2\n",
    "           system = directory_path_1.split('/')[-1]           \n",
    "        \n",
    "           df_full_result_selection = pd.DataFrame()\n",
    "           df_full_cost_selection = pd.DataFrame()\n",
    "           df_full_cost_time_selection = pd.DataFrame()\n",
    "        \n",
    "           \n",
    "           # Cost file processing\n",
    "           for filename in os.listdir(directory):\n",
    "                if \"train_mobiact_fair_metrics_\"  in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    df_to_add_f = pd.read_csv(file_path).iloc[29:79, 1:].abs()\n",
    "                    df_full_result_selection = pd.concat([df_full_result_selection, df_to_add_f], ignore_index=True)\n",
    "                    \n",
    "\n",
    "                if ((\"train_mobiact_cost_metrics_\"  in filename) and (filename.endswith(\"_0.csv\"))):\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    df_to_add_t = pd.read_csv(file_path).iloc[29:79, 1:].abs()\n",
    "                    df_full_cost_time_selection = pd.concat([df_full_cost_time_selection, df_to_add_t], ignore_index=True)\n",
    "    \n",
    "\n",
    "                \n",
    "\n",
    "                if \"train_mobiact_cost_metrics_\"  in filename:\n",
    "                    file_path = os.path.join(directory, filename)\n",
    "                    df_to_add_c = pd.read_csv(file_path).iloc[29:79, 1:].abs()\n",
    "                    df_full_cost_selection = pd.concat([df_full_cost_selection, df_to_add_c], ignore_index=True)\n",
    "           \n",
    "                \n",
    "           for col in ['DI_gender', 'DI_age']:\n",
    "             df_full_result_selection[col] = df_full_result_selection[col].apply(lambda x: abs(1 - x))\n",
    "    \n",
    "           ### Calcul de valeurs moyenne pour le full \n",
    "           result_sel_avg = df_full_result_selection.mean()\n",
    "           cost_sel_avg = df_full_cost_selection.mean()\n",
    "           cost_time_sel_avg = df_full_cost_time_selection.mean()\n",
    "\n",
    "           ## Faire les t-test\n",
    "           # Cost\n",
    "           # time\n",
    "           test_time = evaluate_score(df_full_cost_time_selection['Full_training_time'].to_numpy(), df_full_cost_time['Full_training_time'].to_numpy()) \n",
    "           \n",
    "           # Model quality\n",
    "           # accuracy\n",
    "           test_acc = evaluate_score(df_full_cost['accuracy'].to_numpy(), df_full_cost_selection['accuracy'].to_numpy()) \n",
    "           # F1-score\n",
    "           test_f1 = evaluate_score(df_full_result['F1_score'].to_numpy(), df_full_result_selection['F1_score'].to_numpy()) \n",
    "           #precision\n",
    "           test_precision = evaluate_score(df_full_result['Precision'].to_numpy(), df_full_result_selection['Precision'].to_numpy()) \n",
    "           # recall \n",
    "           test_recall = evaluate_score(df_full_result['Recall'].to_numpy(), df_full_result_selection['Recall'].to_numpy()) \n",
    "\n",
    "           # Fairness\n",
    "           # SPD gender\n",
    "           test_spd_g = evaluate_score(df_full_result_selection['SPD_gender'].to_numpy(), df_full_result['SPD_gender'].to_numpy())\n",
    "           # EOD gender\n",
    "           test_eod_g = evaluate_score(df_full_result_selection['EOD_gender'].to_numpy(), df_full_result['EOD_gender'].to_numpy())\n",
    "           # AOD gender\n",
    "           test_aod_g = evaluate_score(df_full_result_selection['AOD_gender'].to_numpy(), df_full_result['AOD_gender'].to_numpy())\n",
    "           # DI gender\n",
    "           test_di_g = evaluate_score(df_full_result_selection['DI_gender'].to_numpy(), df_full_result['DI_gender'].to_numpy())\n",
    "           # DcI gender\n",
    "           test_dci_g = evaluate_score(df_full_result_selection['DcI_gender'].to_numpy(), df_full_result['DcI_gender'].to_numpy())\n",
    "\n",
    "           # SPD age\n",
    "           test_spd_a = evaluate_score(df_full_result_selection['SPD_age'].to_numpy(), df_full_result['SPD_age'].to_numpy())\n",
    "           # EOD age\n",
    "           test_eod_a = evaluate_score(df_full_result_selection['EOD_age'].to_numpy(), df_full_result['EOD_age'].to_numpy())\n",
    "           # AOD age\n",
    "           test_aod_a = evaluate_score(df_full_result_selection['AOD_age'].to_numpy(), df_full_result['AOD_age'].to_numpy())\n",
    "           # DI age\n",
    "           test_di_a = evaluate_score(df_full_result_selection['DI_age'].to_numpy(), df_full_result['DI_age'].to_numpy())\n",
    "           # DcI age\n",
    "           test_dci_a = evaluate_score(df_full_result_selection['DcI_age'].to_numpy(), df_full_result['DcI_age'].to_numpy())\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "           avg_sel_data = pd.DataFrame({\n",
    "            **{col + '_avg': [result_sel_avg[col]] for col in result_sel_avg.index},\n",
    "            **{col + '_avg': [cost_sel_avg[col]] for col in cost_sel_avg.index},\n",
    "            **{col + '_avg': [cost_time_sel_avg[col]] for col in cost_time_sel_avg.index},\n",
    "           'dataset': ['mobiact'],\n",
    "           'model': [model],\n",
    "           'system': [system],\n",
    "           'ratio': [directory_path_2.split('_')[-1]],\n",
    "           'test_time' : [test_time], \n",
    "           'test_acc' : [test_acc], \n",
    "           'test_f1' : [test_f1], \n",
    "           'test_precision' : [test_precision], \n",
    "           'test_recall' : [test_recall],\n",
    "           'test_SPD_gender' : [test_spd_g], \n",
    "           'test_EOD_gender' : [test_eod_g], \n",
    "           'test_AOD_gender' : [test_aod_g], \n",
    "           'test_DI_gender' : [test_di_g], \n",
    "           'test_DcI_gender' : [test_dci_g], \n",
    "\n",
    "           'test_SPD_age' : [test_spd_a],\n",
    "           'test_EOD_age' : [test_eod_a],\n",
    "           'test_AOD_age' : [test_aod_a], \n",
    "           'test_DI_age' : [test_di_a], \n",
    "           'test_DcI_age' : [test_dci_a], \n",
    "\n",
    "           'test_SPD_race' : ' ',\n",
    "           'test_EOD_race' : ' ',\n",
    "           'test_AOD_race' : ' ', \n",
    "           'test_DI_race' : ' ',\n",
    "           'test_DcI_race' : ' ',\n",
    "            })\n",
    "           \n",
    "           df_average_results = pd.concat([df_average_results, avg_sel_data], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "df_average_results.to_csv('../results/test/mobiact_ttest_epochs_intervals.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-processing T-test results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering T-test results based on model utility and fairness constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on cases where model accuracy is severely degraded, in such cases, fairness impact is considered insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def false_positives(path= '../results/test/ars_ttest_results_epochs_intervals.csv', x = 10):\n",
    "    # Initialize the reference accuracy\n",
    "    ref_accuracy = None\n",
    "    # DataFrame to hold rows that are 'false better'\n",
    "    false_better_df = pd.DataFrame()\n",
    "\n",
    "    df = pd.read_csv(path) \n",
    "\n",
    "    # Columns to check for the value 'positive'\n",
    "    columns_to_modify = ['test_SPD_gender', 'test_EOD_gender', 'test_AOD_gender', 'test_DI_gender', 'test_DcI_gender', \n",
    "                        'test_SPD_age', 'test_EOD_age', 'test_AOD_age', 'test_DI_age', 'test_DcI_age', \n",
    "                        'test_SPD_race', 'test_EOD_race', 'test_AOD_race', 'test_DI_race', 'test_DcI_race']\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['system'] == 'Full':\n",
    "            ref_accuracy = row['accuracy_avg']  # Set the reference accuracy\n",
    "        elif ref_accuracy is not None:  # Ensure there is a reference to compare against\n",
    "            current_accuracy = row['accuracy_avg']\n",
    "            if current_accuracy <= ref_accuracy - x and row['system'] != 'Random' and row['ratio'] != 0.5:\n",
    "                # Add row to DataFrame using pd.concat\n",
    "                false_better_df = pd.concat([false_better_df, pd.DataFrame([row])], ignore_index=True)\n",
    "                # Modify specific columns if their values are 'positive'\n",
    "                for col in columns_to_modify:\n",
    "                    if row[col] == 'positive':\n",
    "                        df.at[index, col] = 'insignificant-p'\n",
    "\n",
    "\n",
    "    # Save 'false better' rows to a new CSV file\n",
    "    path_o= '../results/test/false_better_' + str(x) + '_' + path.split('/')[1]\n",
    "    false_better_df.to_csv(path_o, index=False)\n",
    "\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    path_m= '../results/test/positive_insig_'+ str(x) + '_' + path.split('/')[1]\n",
    "    df.to_csv(path_m, index=False)\n",
    "\n",
    "    return len(false_better_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "ars = false_positives('../results/test/ars_ttest_results_epochs_intervals.csv', k)\n",
    "dc = false_positives('../results/test/dc_ttest_results_epochs_intervals.csv', k)\n",
    "kdd = false_positives('../results/test/kdd_ttest_results_epochs_intervals.csv', k)\n",
    "adult = false_positives('../results/test/adult_ttest_results_epochs_intervals.csv', k)\n",
    "mobiact = false_positives('../results/test/mobiact_ttest_epochs_intervals.csv', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairness constraint: the impact is considered significant (positive or negative) if the difference between the full metric and the selection metric exceeds 1%, or 0.01 for DI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def false_positives_fairness(path=None, thr_di=1, thr_other=0.01, output_false_positives=None, output=None):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Initialize DataFrames\n",
    "    false_better_df = pd.DataFrame()\n",
    "\n",
    "    # Columns to check for the value 'positive'\n",
    "    columns_to_modify = [\n",
    "        'test_SPD_gender', 'test_EOD_gender', 'test_AOD_gender', 'test_DI_gender', 'test_DcI_gender', \n",
    "        'test_SPD_age', 'test_EOD_age', 'test_AOD_age', 'test_DI_age', 'test_DcI_age', \n",
    "        'test_SPD_race', 'test_EOD_race', 'test_AOD_race', 'test_DI_race', 'test_DcI_race'\n",
    "    ]\n",
    "\n",
    "    # Columns with the average metrics\n",
    "    columns_to_check = [\n",
    "        'SPD_gender_avg', 'EOD_gender_avg', 'AOD_gender_avg', 'DI_gender_avg', 'DcI_gender_avg',\n",
    "        'SPD_age_avg', 'EOD_age_avg', 'AOD_age_avg', 'DI_age_avg', 'DcI_age_avg',\n",
    "        'SPD_race_avg', 'EOD_race_avg', 'AOD_race_avg', 'DI_race_avg', 'DcI_race_avg'\n",
    "    ]\n",
    "\n",
    "    # Initialize references dictionary\n",
    "    references = {col: None for col in columns_to_check if col in df.columns}\n",
    "\n",
    "    # Iterate through DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        if row['system'] == 'Full':\n",
    "            references = {col: None for col in columns_to_check if col in df.columns}\n",
    "            # Set the reference values for metrics when system is 'Full'\n",
    "            for col in columns_to_check:\n",
    "                if col in df.columns:\n",
    "                    references[col] = row[col]\n",
    "        else:\n",
    "            # Check other rows where system is not 'Full'\n",
    "            for test_col, avg_col in zip(columns_to_modify, columns_to_check):\n",
    "                if test_col in df.columns and avg_col in df.columns and row[test_col] == 'positive':\n",
    "                    threshold = thr_di if (('DI_gender_avg' in avg_col) or ('DI_age_avg' in avg_col) or ('DI_race_avg' in avg_col)) else thr_other  # Set threshold based on DI metrics\n",
    "                    if references.get(avg_col) is not None and (references[avg_col] - row[avg_col] < threshold):\n",
    "                        df.at[index, test_col] = 'insignificant-p2'\n",
    "                        if row['system'] != 'Random' and row['ratio'] != 0.5:\n",
    "                            # Add row to 'false better' DataFrame if certain conditions are met\n",
    "                            false_better_df = pd.concat([false_better_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    # Save 'false better' rows to a new CSV file\n",
    "    false_better_df.to_csv(output_false_positives, index=False)\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output, index=False)\n",
    "\n",
    "    return len(false_better_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['../results/test/positive_insig_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "results_paths = ['../results/test/positive_insig_2_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "ars = false_positives_fairness(path=file_paths[0], thr_di=1, thr_other=0.01, output_false_positives='../results/test/ars_false_positives_with_regard_to_fairness.csv',output=results_paths[0] )\n",
    "dc = false_positives_fairness(path=file_paths[1], thr_di=1, thr_other=0.01,output_false_positives='../results/test/dc_false_positives_with_regard_to_fairness.csv', output=results_paths[1])\n",
    "adult = false_positives_fairness(path=file_paths[2], thr_di=1, thr_other=0.01,output_false_positives='../results/test/adult_false_positives_with_regard_to_fairness.csv', output=results_paths[2])\n",
    "kdd = false_positives_fairness(path=file_paths[3], thr_di=1, thr_other=0.01,output_false_positives='../results/test/kdd_false_positives_with_regard_to_fairness.csv', output=results_paths[3])\n",
    "mobiact = false_positives_fairness(path=file_paths[4], thr_di=1, thr_other=0.01,output_false_positives='../results/test/mobiact_false_positives_with_regard_to_fairness.csv', output=results_paths[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concat_filter_csv(file_paths, output):\n",
    "    # Définition des colonnes communes à extraire de chaque fichier\n",
    "    common_columns = ['dataset', 'model', 'system', 'ratio', 'test_time', 'test_acc', \n",
    "                      'test_f1', 'test_precision', 'test_recall', 'test_SPD_gender', \n",
    "                      'test_EOD_gender', 'test_AOD_gender', 'test_DI_gender', 'test_DcI_gender', \n",
    "                      'test_SPD_age', 'test_EOD_age', 'test_AOD_age', 'test_DI_age', 'test_DcI_age', \n",
    "                      'test_SPD_race', 'test_EOD_race', 'test_AOD_race', 'test_DI_race', 'test_DcI_race']\n",
    "\n",
    "    # Liste pour stocker les DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Boucle sur chaque chemin de fichier dans la liste\n",
    "    for path in file_paths:\n",
    "        # Lecture du fichier CSV\n",
    "        df = pd.read_csv(path)\n",
    "        # Extraction des colonnes communes\n",
    "        df_common = df[common_columns]\n",
    "        # Ajout du DataFrame extrait à la liste\n",
    "        df_list.append(df_common)\n",
    "\n",
    "    # Concaténation des DataFrames\n",
    "    result_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Remplacement de 'significant-p' par 'significant'\n",
    "    result_df.replace('significant-p2', 'significant', inplace=True)\n",
    "    result_df.to_csv(output + '-w-significant-p2.csv', index=False)\n",
    "\n",
    "    # Filtrage des lignes où 'ratio' est '0.5' ou 'system' est 'Random'\n",
    "    result_df = result_df[(result_df['ratio'] != 0.5) & (result_df['system'] != 'Full')]\n",
    "    result_df.to_csv(output+'-2c-w-random.csv', index=False)\n",
    "\n",
    "    result_df = result_df[(result_df['system'] != 'Random')]\n",
    "    result_df.to_csv(output+'-2C-wo-random.csv', index=False)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['../results/test/positive_insig_2_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "output = '../results/test/ttest_5'\n",
    "concat_filter_csv(file_paths, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility constraint: the impact is considered significant (positive or negative) if the difference between the full metric and the selection metric exceeds 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def false_positives_utility(path=None,thr_acc=1, thr=0.01, output_false_positives=None, output=None):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Initialize DataFrames\n",
    "    false_better_df = pd.DataFrame()\n",
    "\n",
    "    # Columns to check for the value 'positive'\n",
    "    columns_to_modify = [\n",
    "        'test_acc', 'test_f1', 'test_precision', 'test_recall'\n",
    "    ]\n",
    "\n",
    "    # Columns with the average metrics\n",
    "    columns_to_check = ['accuracy_avg', 'F1_score_avg', 'Precision_avg', 'Recall_avg'\n",
    "        \n",
    "    ]\n",
    "\n",
    "    # Initialize references dictionary\n",
    "    references = {col: None for col in columns_to_check if col in df.columns}\n",
    "\n",
    "    # Iterate through DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        if row['system'] == 'Full':\n",
    "            references = {col: None for col in columns_to_check if col in df.columns}\n",
    "            # Set the reference values for metrics when system is 'Full'\n",
    "            for col in columns_to_check:\n",
    "                if col in df.columns:\n",
    "                    references[col] = row[col]\n",
    "        else:\n",
    "            # Check other rows where system is not 'Full'\n",
    "            for test_col, avg_col in zip(columns_to_modify, columns_to_check):\n",
    "                if test_col in df.columns and avg_col in df.columns and row[test_col] == 'positive':\n",
    "                    threshold = thr_acc if (('accuracy_avg' in avg_col)) else thr  # Set threshold based on DI metrics\n",
    "\n",
    "                    if references.get(avg_col) is not None and (abs(references[avg_col] - row[avg_col]) < threshold):\n",
    "                        df.at[index, test_col] = 'insignificant-pu'\n",
    "                        if row['system'] != 'Random' and row['ratio'] != 0.5:\n",
    "                            # Add row to 'false better' DataFrame if certain conditions are met\n",
    "                            false_better_df = pd.concat([false_better_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    # Save 'false better' rows to a new CSV file\n",
    "    false_better_df.to_csv(output_false_positives, index=False)\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output, index=False)\n",
    "\n",
    "    return len(false_better_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['../results/test/positive_insig_2_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_2_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "results_paths =  ['../results/test/positive_insig_3_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_3_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_3_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_3_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_3_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "ars = false_positives_utility(path=file_paths[0], thr_acc=1, thr=0.01, output_false_positives='../results/test/ars_false_positives_utility_with_regard_to_utility.csv',output=results_paths[0] )\n",
    "dc = false_positives_utility(path=file_paths[1], thr_acc=1, thr=0.01,output_false_positives='../results/test/dc_false_positives_utility_with_regard_to_utility.csv', output=results_paths[1])\n",
    "adult = false_positives_utility(path=file_paths[2], thr_acc=1, thr=0.01,output_false_positives='../results/test/adult_false_positives_utility_with_regard_to_utility.csv', output=results_paths[2])\n",
    "kdd = false_positives_utility(path=file_paths[3], thr_acc=1, thr=0.01,output_false_positives='../results/test/kdd_false_positives_utility_with_regard_to_utility.csv', output=results_paths[3])\n",
    "mobiact = false_positives_utility(path=file_paths[4], thr_acc=1, thr=0.01,output_false_positives='../results/test/mobiact_false_positives_utility_with_regard_to_utility.csv', output=results_paths[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditions on both positive and negative impact "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def false_positives_fairness(path=None, thr_di=1, thr_other=0.01, output_false_positives=None, output=None):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Initialize DataFrames\n",
    "    false_better_df = pd.DataFrame()\n",
    "\n",
    "    # Columns to check for the value 'positive'\n",
    "    columns_to_modify = [\n",
    "        'test_SPD_gender', 'test_EOD_gender', 'test_AOD_gender', 'test_DI_gender', 'test_DcI_gender', \n",
    "        'test_SPD_age', 'test_EOD_age', 'test_AOD_age', 'test_DI_age', 'test_DcI_age', \n",
    "        'test_SPD_race', 'test_EOD_race', 'test_AOD_race', 'test_DI_race', 'test_DcI_race'\n",
    "    ]\n",
    "\n",
    "    # Columns with the average metrics\n",
    "    columns_to_check = [\n",
    "        'SPD_gender_avg', 'EOD_gender_avg', 'AOD_gender_avg', 'DI_gender_avg', 'DcI_gender_avg',\n",
    "        'SPD_age_avg', 'EOD_age_avg', 'AOD_age_avg', 'DI_age_avg', 'DcI_age_avg',\n",
    "        'SPD_race_avg', 'EOD_race_avg', 'AOD_race_avg', 'DI_race_avg', 'DcI_race_avg'\n",
    "    ]\n",
    "\n",
    "    # Initialize references dictionary\n",
    "    references = {col: None for col in columns_to_check if col in df.columns}\n",
    "\n",
    "    # Iterate through DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        if row['system'] == 'Full':\n",
    "            references = {col: None for col in columns_to_check if col in df.columns}\n",
    "            # Set the reference values for metrics when system is 'Full'\n",
    "            for col in columns_to_check:\n",
    "                if col in df.columns:\n",
    "                    references[col] = row[col]\n",
    "        else:\n",
    "            # Check other rows where system is not 'Full'\n",
    "            for test_col, avg_col in zip(columns_to_modify, columns_to_check):\n",
    "                if test_col in df.columns and avg_col in df.columns and row[test_col] == 'negative':\n",
    "                    threshold = thr_di if (('DI_gender_avg' in avg_col) or ('DI_age_avg' in avg_col) or ('DI_race_avg' in avg_col)) else thr_other  # Set threshold based on DI metrics\n",
    "                    if references.get(avg_col) is not None and (abs(references[avg_col] - row[avg_col]) < threshold):\n",
    "                        df.at[index, test_col] = 'insignificant-p3'\n",
    "                        if row['system'] != 'Random' and row['ratio'] != 0.5:\n",
    "                            # Add row to 'false better' DataFrame if certain conditions are met\n",
    "                            false_better_df = pd.concat([false_better_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    # Save 'false better' rows to a new CSV file\n",
    "    false_better_df.to_csv(output_false_positives, index=False)\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output, index=False)\n",
    "\n",
    "    return len(false_better_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths =  ['../results/test/positive_insig_3_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_3_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_3_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_3_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_3_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "results_paths =  ['../results/test/positive_insig_4_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_4_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_4_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_4_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_4_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "ars = false_positives_fairness(path=file_paths[0], thr_di=1, thr_other=0.01, output_false_positives='../results/test/ars_false_positives_with_regard_to_negative_fairness.csv',output=results_paths[0] )\n",
    "dc = false_positives_fairness(path=file_paths[1], thr_di=1, thr_other=0.01,output_false_positives='../results/test/dc_false_positives_with_regard_to_negative_fairnessn.csv', output=results_paths[1])\n",
    "adult = false_positives_fairness(path=file_paths[2], thr_di=1, thr_other=0.01,output_false_positives='../results/test/adult_false_positives_with_regard_to_negative_fairness.csv', output=results_paths[2])\n",
    "kdd = false_positives_fairness(path=file_paths[3], thr_di=1, thr_other=0.01,output_false_positives='../results/test/kdd_false_positives_with_regard_to_negative_fairness.csv', output=results_paths[3])\n",
    "mobiact = false_positives_fairness(path=file_paths[4], thr_di=1, thr_other=0.01,output_false_positives='../results/test/mobiact_false_positives_with_regard_to_negative_fairness.csv', output=results_paths[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def false_positives_utility(path=None,thr_acc=1, thr=0.01, output_false_positives=None, output=None):\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Initialize DataFrames\n",
    "    false_better_df = pd.DataFrame()\n",
    "\n",
    "    # Columns to check for the value 'positive'\n",
    "    columns_to_modify = [\n",
    "        'test_acc', 'test_f1', 'test_precision', 'test_recall'\n",
    "    ]\n",
    "\n",
    "    # Columns with the average metrics\n",
    "    columns_to_check = ['accuracy_avg', 'F1_score_avg', 'Precision_avg', 'Recall_avg'\n",
    "        \n",
    "    ]\n",
    "\n",
    "    # Initialize references dictionary\n",
    "    references = {col: None for col in columns_to_check if col in df.columns}\n",
    "\n",
    "    # Iterate through DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        if row['system'] == 'Full':\n",
    "            references = {col: None for col in columns_to_check if col in df.columns}\n",
    "            # Set the reference values for metrics when system is 'Full'\n",
    "            for col in columns_to_check:\n",
    "                if col in df.columns:\n",
    "                    references[col] = row[col]\n",
    "        else:\n",
    "            # Check other rows where system is not 'Full'\n",
    "            for test_col, avg_col in zip(columns_to_modify, columns_to_check):\n",
    "                if test_col in df.columns and avg_col in df.columns and row[test_col] == 'negative':\n",
    "                    threshold = thr_acc if (('accuracy_avg' in avg_col)) else thr  # Set threshold based on DI metrics\n",
    "\n",
    "                    if references.get(avg_col) is not None and (abs(references[avg_col] - row[avg_col]) < threshold):\n",
    "                        df.at[index, test_col] = 'insignificant-pu2'\n",
    "                        if row['system'] != 'Random' and row['ratio'] != 0.5:\n",
    "                            # Add row to 'false better' DataFrame if certain conditions are met\n",
    "                            false_better_df = pd.concat([false_better_df, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    # Save 'false better' rows to a new CSV file\n",
    "    false_better_df.to_csv(output_false_positives, index=False)\n",
    "\n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output, index=False)\n",
    "\n",
    "    return len(false_better_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = ['../results/test/positive_insig_4_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_4_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_4_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_4_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_4_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "results_paths =  ['../results/test/positive_insig_5_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "ars = false_positives_utility(path=file_paths[0], thr_acc=1, thr=0.01, output_false_positives='../results/test/ars_false_negatives_utility_with_regard_to_utility.csv',output=results_paths[0] )\n",
    "dc = false_positives_utility(path=file_paths[1], thr_acc=1, thr=0.01,output_false_positives='../results/test/dc_false_negatives_utility_with_regard_to_utility.csv', output=results_paths[1])\n",
    "adult = false_positives_utility(path=file_paths[2], thr_acc=1, thr=0.01,output_false_positives='../results/test/adult_false_negatives_utility_with_regard_to_utility.csv', output=results_paths[2])\n",
    "kdd = false_positives_utility(path=file_paths[3], thr_acc=1, thr=0.01,output_false_positives='../results/test/kdd_false_negatives_utility_with_regard_to_utility.csv', output=results_paths[3])\n",
    "mobiact = false_positives_utility(path=file_paths[4], thr_acc=1, thr=0.01,output_false_positives='../results/test/mobiact_false_negatives_utility_with_regard_to_utility.csv', output=results_paths[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concat outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def concat_filter_csv(file_paths, output):\n",
    "    # Définition des colonnes communes à extraire de chaque fichier\n",
    "    common_columns = ['dataset', 'model', 'system', 'ratio', 'test_time', 'test_acc', \n",
    "                      'test_f1', 'test_precision', 'test_recall', 'test_SPD_gender', \n",
    "                      'test_EOD_gender', 'test_AOD_gender', 'test_DI_gender', 'test_DcI_gender', \n",
    "                      'test_SPD_age', 'test_EOD_age', 'test_AOD_age', 'test_DI_age', 'test_DcI_age', \n",
    "                      'test_SPD_race', 'test_EOD_race', 'test_AOD_race', 'test_DI_race', 'test_DcI_race']\n",
    "\n",
    "    # Liste pour stocker les DataFrames\n",
    "    df_list = []\n",
    "\n",
    "    # Boucle sur chaque chemin de fichier dans la liste\n",
    "    for path in file_paths:\n",
    "        # Lecture du fichier CSV\n",
    "        df = pd.read_csv(path)\n",
    "        # Extraction des colonnes communes\n",
    "        df_common = df[common_columns]\n",
    "        # Ajout du DataFrame extrait à la liste\n",
    "        df_list.append(df_common)\n",
    "\n",
    "    # Concaténation des DataFrames\n",
    "    result_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    # Remplacement de 'significant-p' par 'significant'\n",
    "    result_df.to_csv(output + '-w-significant-p-5C.csv', index=False)\n",
    "\n",
    "    result_df.replace('insignificant-p', 'insignificant', inplace=True)\n",
    "\n",
    "    result_df.replace('insignificant-p2', 'insignificant', inplace=True)\n",
    "\n",
    "    result_df.replace('insignificant-p3', 'insignificant', inplace=True)\n",
    "\n",
    "    result_df.replace('insignificant-pu', 'insignificant', inplace=True)\n",
    "\n",
    "    result_df.replace('insignificant-pu2', 'insignificant', inplace=True)\n",
    "\n",
    "    result_df.to_csv(output + '-wo-significant-p-5C.csv', index=False)\n",
    "\n",
    "    # Filtrage des lignes où 'ratio' est '0.5' ou 'system' est 'Random'\n",
    "    result_df = result_df[(result_df['ratio'] != 0.5) & (result_df['system'] != 'Full')]\n",
    "    result_df.to_csv(output+'-5c-w-random.csv', index=False)\n",
    "\n",
    "    result_df = result_df[(result_df['system'] != 'Random')]\n",
    "    result_df.to_csv(output+'-5C-wo-random.csv', index=False)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths =  ['../results/test/positive_insig_5_5_ars_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_5_dc_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_5_mobiact_ttest_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_5_adult_ttest_results_epochs_intervals.csv', \n",
    "              '../results/test/positive_insig_5_5_kdd_ttest_results_epochs_intervals.csv']\n",
    "\n",
    "output = '../results/test/ttest_5'\n",
    "concat_filter_csv(file_paths, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
